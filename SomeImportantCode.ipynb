{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Dataset & Problem Statment\n",
    "def get_data(size= 10000):\n",
    "    df = pd.DataFrame()\n",
    "    size = 10000\n",
    "    df['age'] = np.random.randint(0,100,size)\n",
    "    df['time_in_bed'] = np.random.randint(0,9,size)\n",
    "    df['pct_sleeping'] = np.random.randint(size)\n",
    "    df['favorite_food'] = np.random.choice(['pizza','ice-cream','burger','rice'], size)\n",
    "    df['hate_food'] = np.random.choice(['milk','vegetables','eggs'])\n",
    "    return df\n",
    "\n",
    "df = get_data()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly = df.groupby(df.MeterId).resample('60min', on='Usagedate')['Total'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"path\", parse_dates=['Usagedate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ele = ele.drop(['measType', 'valStat'],axis= 1)\n",
    "ele.rename(columns = {'interval': 'Usagedate', 'kwhVal':'Total'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Electricity--- Date Range {ele['Usagedate'].min()} till {ele['Usagedate'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wea_copy['m-y']=wea_copy['Usagedate'].dt.strftime('%m-%y')\n",
    "wea_temp=wea_copy.groupby('m-y').agg({'Temp':'mean'}).reset_index()\n",
    "wea_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot line graph on axis #1\n",
    "def plot_total_monthly(df1 = ele,df2 = wea):\n",
    "    plt.figure(figsize=(20,10))\n",
    "    ax1 = sns.boxplot(x='m-y', y='Total', data=df1, color='orange', showfliers=False, showmeans=True)\n",
    "    ax1.set_xlabel('Months',fontsize=15)\n",
    "    ax1.set_ylabel('Monthly Total Usage by Meter',fontsize=15)\n",
    "    ax1_patch = mpatches.Patch(color='orange', label='monthly total usage by Meter')\n",
    "    ax1.legend(handles=[ax1_patch], loc=\"upper left\")\n",
    "    ax2 = ax1.twinx()\n",
    "    # plot bar graph on axis #2\n",
    "    sns.lineplot(x='m-y', y='Temp', data=df2, color='blue', ax = ax2 )  # Pre-existing axes for the plot\n",
    "    ax2.grid(b=False) # turn off grid #2\n",
    "    ax2.set_ylabel('Temp(C)',fontsize=15)\n",
    "    ax2_patch = mpatches.Patch(color='blue', label='average temperature in fahrenheit')\n",
    "    ax2.legend(handles=[ax2_patch], loc=\"upper right\")\n",
    "    plt.title('Variation of Total Usage for all meters',fontsize=15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns=['_'.join(col).strip() for col in df.columns.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#isnull electricity\n",
    "ele.isnull().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_dt = ele_copy.Usagedate.min()\n",
    "max_dt = ele_copy.Usagedate.max()\n",
    "print(\"Range of datetimes: {} to {}\".format(min_dt, max_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daily=df.set_index('Usagedate')\n",
    "df_daily=df_daily.groupby('MeterId').resample('D').agg({\n",
    "    'Total' : 'sum',\n",
    "    'Cooling': 'sum',\n",
    "    'HVAC Cooling' : 'sum',\n",
    "    'Temperature' : 'mean',\n",
    "    'in.sqft' : 'first',\n",
    "    'in.hvac_cooling_efficiency' : 'first',\n",
    "    'in.hvac_cooling_type' : 'first',\n",
    "    'in.occupants' : 'mean'\n",
    "}).reset_index(drop=False)\n",
    "df_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_monthly=df.set_index('Usagedate')\n",
    "\n",
    "df_monthly=df_monthly.groupby('MeterId').resample('M').agg({\n",
    "    'Total' : 'sum',\n",
    "    'Cooling': 'sum',\n",
    "    'HVAC Cooling' : 'sum',\n",
    "    'Temperature' : 'mean',\n",
    "    'in.hvac_cooling_efficiency' : 'first',\n",
    "    'in.hvac_cooling_type' : 'first',\n",
    "    'in.occupants' : 'mean'\n",
    "}).reset_index(drop=False)\n",
    "df_monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_data(df, Usagedate):\n",
    "    #df['year']=df[Usagedate].dt.year\n",
    "    df['month']=df[Usagedate].dt.month\n",
    "    df['day']=df[Usagedate].dt.strftime(\"%d\")\n",
    "    df['hour']=df[Usagedate].dt.hour\n",
    "    df['is_wkday']=np.where(df[Usagedate].dt.dayofweek<5,1,0)   # 1 wkday, 0 wkend\n",
    "    #df['m-y'] = df[Usagedate].dt.strftime(\"%m-%y\")\n",
    "    df['is_spring']=np.where(((df['month']>=3)&(df['month']<=5)),1,0)\n",
    "    df['is_summer']=np.where(((df['month']>=6)&(df['month']<=8)),1,0)\n",
    "    df['is_fall']=np.where(((df['month']>=9)&(df['month']<=10)),1,0)\n",
    "    df['is_winter']=np.where(((df['month']==11)|(df['month']==12)|(df['month']==1)|(df['month']==2)),1,0)\n",
    "    df['is_night']=np.where(((df['hour']>=22)|(df['hour']<=6)),1,0)\n",
    "    df['is_morning']=np.where(((df['hour']>6)&(df['hour']<=12)),1,0)\n",
    "    df['is_afternoon']=np.where(((df['hour']>12)&(df['hour']<=16)),1,0)\n",
    "    df['is_evening']=np.where(((df['hour']>16)&(df['hour']<=21)),1,0)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source.groupby(['Country','City']).agg(\n",
    "    mod=('Short name', lambda x: x.value_counts().index[0]),\n",
    "    avg=('account', 'mean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_cal(row):\n",
    "    if row['age'] >=90:\n",
    "        return row['favorite_food'] \n",
    "    if (row['time_in_bed'] > 5) & (row['pct_sleeping']>0.5):\n",
    "        return row['favorite_food']\n",
    "    return row['hate_food']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for loops to iterate over each row of the data frame.\n",
    "%%timeit\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "   df.loc[index,'reward'] = reward_cal(row)\n",
    "# above code is taking long time\n",
    "# for better code\n",
    "%%timeit\n",
    "df['reward'] = df.apply(reward_cal, axis = 1)\n",
    "\n",
    "#Same in vectorization\n",
    "%%timeit\n",
    "\n",
    "df['reward'] = df['hate_food']\n",
    "df.loc[((df['pct_sleeping']>0.5) &(df['time_in_bed']>5))| (df['age']>90), 'reward'] = df['favorite_food']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enumerate(): enumerate creates an index item pair for each item in the object provided.\n",
    "letters = ['a', 'b', 'c', 'd' ]\n",
    "indexed_letters = enumerate(letters)\n",
    "indexed_letters_list = list(indexed_letters)\n",
    "# [(5, 'a'), (6, 'b'), (7, 'c'), (8, 'd')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing values in a DataFrame is a very important task, especially in the data cleaning phase. \n",
    "start_time = time.time()\n",
    "names['Gender'].loc[names.Gender=='female'] = 'FEMALE'\n",
    "end_time = time.time()\n",
    "\n",
    "names['Gender'].replace('female', 'FEMALE', inplace=True)  # faster than above appraoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map(): The last notable built-in function we'll cover is the map() function. \n",
    "# map applies a function to each element in an object. Notice that the map \n",
    "# function takes two arguments; first, the function you'd like to apply, and \n",
    "# second, the object you'd like to apply that function on.\n",
    "nums = [1.5, 2.3, 3.4, 4.6, 5.0]\n",
    "rnd_nums = map(round, nums)\n",
    "print(list(rnd_nums))\n",
    "[2, 2, 3, 5, 5]\n",
    "\n",
    "\n",
    "# map() with lambda \n",
    "nums = [1, 2, 3, 4, 5]\n",
    "sqrd_nums = map(lambda x: x ** 2, nums)\n",
    "print(list(sqrd_nums))\n",
    "[1, 4, 9, 16, 25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EV_hourly[(EV_hourly.MeterId == 1004300886) & (EV_hourly.month == 12)].head(24*30)\n",
    "[[\"Usagedate\", \"Total\"]].plot(x=\"Usagedate\", y=\"Total\", figsize=(15, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaps_counter_yearly = pd.DataFrame(\n",
    "    columns=[\"MeterId\", \"months\", \"num_gaps\", \"total_gaps_len\" , \"mean_gaps_len\", \n",
    "             \"max_gap_len\", \"min_gap_date\", \"max_gap_date\",  \"total_usage_gaps\", \n",
    "             \"avg_usage_gaps\" ,\"max_usage_gap\"],\n",
    ")\n",
    "\n",
    "def count_gap_yearly(meter):\n",
    "    temp = EV_hourly[(EV_hourly.MeterId == meter)].reset_index()\n",
    "    temp.Total = np.where(temp.Total < 0.1, 0, temp.Total)\n",
    "    temp.sort_values(by=['Usagedate'],ascending=True, inplace=True)\n",
    "    idx_1 = temp[temp.Total > 0].index.values\n",
    "    idx_diff = np.diff(idx_1)\n",
    "    prept = []\n",
    "    postpt = []\n",
    "    time = []\n",
    "    total_power = []\n",
    "    flag = 0\n",
    "    for i in range(len(idx_diff)):\n",
    "        if ((i+1) == len(idx_diff)):\n",
    "            if (temp.loc[idx_1[i+1],'Total'] > 0):\n",
    "                postpt.append(idx_1[i+1])\n",
    "            break\n",
    "        if (idx_diff[i] == 1) & (flag == 0):\n",
    "            prept.append(idx_1[i])\n",
    "            flag = 1\n",
    "        if (idx_diff[i] == 1) & (flag == 1):\n",
    "            continue\n",
    "        if (idx_diff[i] != 1) & (flag == 1):\n",
    "            postpt.append(idx_1[i])\n",
    "            flag = 0       \n",
    "    prept = np.array(prept)\n",
    "    postpt = np.array(postpt)\n",
    "    segNum = len(prept)  #number of segment\n",
    "    for k in range(segNum):\n",
    "        n = temp.loc[postpt[k],'Usagedate'] - temp.loc[prept[k],'Usagedate']\n",
    "        time.append(n)\n",
    "    total_time = np.sum(time)  \n",
    "    mean_time = np.mean(time)\n",
    "    max_time = np.max(time)\n",
    "    mt = np.argmax(time)\n",
    "    min_t = temp.loc[prept[mt],'Usagedate']\n",
    "    max_t = temp.loc[postpt[mt],'Usagedate']\n",
    "    for k in range(segNum):\n",
    "        x = prept[k]\n",
    "        y = postpt[k]\n",
    "        tt = []\n",
    "        for r in range(x, (y+1)):\n",
    "            tt.append(temp.loc[r,'Total'])\n",
    "        total_power.append(np.sum(tt))\n",
    "    total_power = np.array(total_power)\n",
    "    total_usage = np.sum(total_power)\n",
    "    mean_usage = np.mean(total_power)\n",
    "    max_usage = np.max(total_power)\n",
    "    mon = temp.month.unique()\n",
    "    global gaps_counter_yearly\n",
    "    gaps_counter_yearly = gaps_counter_yearly.append(\n",
    "            {\n",
    "                \"MeterId\": met,\n",
    "                \"months\": mon,\n",
    "                \"num_gaps\": segNum ,\n",
    "                \"total_gaps_len\": total_time,\n",
    "                \"mean_gaps_len\": mean_time,\n",
    "                \"max_gap_len\":  max_time,\n",
    "                \"min_gap_date\":  min_t,\n",
    "                \"max_gap_date\":  max_t,\n",
    "                \"total_usage_gaps\": total_usage,\n",
    "                \"avg_usage_gaps\": mean_usage,\n",
    "                \"max_usage_gap\": max_usage,          \n",
    "            },\n",
    "            ignore_index=True\n",
    "        )\n",
    "    return [segNum, total_time, mean_time, max_time, min_t, max_t, total_usage, mean_usage, max_usage]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def month_plot(meterid, mon):\n",
    "    temp=EV_hourly[(EV_hourly['MeterId']==meterid) & (EV_hourly['month'] == mon)].reset_index()\n",
    "    temp.sort_values(by=['Usagedate'],ascending=True,inplace=True)\n",
    "    fig = px.line(temp, x=\"Usagedate\", y=['Total'],\n",
    "                  title=\"Plot for \"+str(meterid)+\"\").update_layout(yaxis_title=\"Electricity Usage\",xaxis_title=\"Date\")\n",
    "    fig.update_xaxes(rangeslider_visible=True)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def year_plot(meterid):\n",
    "    temp=EV_hourly[EV_hourly['MeterId']==meterid].reset_index()\n",
    "    temp.sort_values(by=['Usagedate'],ascending=True,inplace=True)\n",
    "    fig = px.line(temp, x=\"Usagedate\", y=['Total'],\n",
    "                  title=\"Plot for \"+str(meterid)+\"\").update_layout(yaxis_title=\"Electricity Usage\",xaxis_title=\"Date\")\n",
    "    fig.update_xaxes(rangeslider_visible=True)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='MeterId', y='Total', data=ele_monthly[ele_monthly['MeterId'] == met], hue='is_wkday').set_title(f\"Meter {met}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(30,10))\n",
    "ax.plot(ele_wkdy_monthly['m-y'],ele_wkdy_monthly['Total'],color='blue',label=\"weekday\")\n",
    "ax.plot(ele_wkend_monthly['m-y'],ele_wkend_monthly['Total'],color='olive',label=\"weekend\")\n",
    "ax.set_xlabel('Month-Year') \n",
    "ax.set_ylabel('kWh')\n",
    "plt.title('Avg monthly total Energy Usage')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ele1 = ele_season[['MeterId_', 'is_wkday_', 'is_night_', 'Total_mean']][ele_season['is_summer_'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w = ele_ncopy.join(wea, on=['Usagedate'], how='left').reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w['Temp'] = df_w['Temp'].bfill()\n",
    "df_w['Temp'] = df_w['Temp'].ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w.set_index('Usagedate',inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_values = [25000, 30000]\n",
    "df2 = df[df['Fee'].isin(list_of_values)]\n",
    "\n",
    "# Using isin([]) method to get opposite rows\n",
    "df2 = df[~df['Fee'].isin([25000, 30000])]\n",
    "\n",
    "# Using DataFrame.query() method to select rows\n",
    "df2 = df.query('Discount in [1000,2000]')\n",
    "\n",
    "# Filter Rows by list of values\n",
    "df2 = df.query('Discount == [1000,2000]')\n",
    "\n",
    "# Select list of rows using variable\n",
    "list_of_value = [1000,2000]\n",
    "df2 = df.query('Discount in @list_of_value')\n",
    "\n",
    "# Using DataFrame.query() method\n",
    "list_of_value = [1000,2000]\n",
    "df2 = df.query('Discount == @list_of_value')\n",
    "\n",
    "# Select rows using DataFrame.loc[] to apply() and lambda function \n",
    "df2 = df.loc[df.apply(lambda x: x.Fee in [20000,22000], axis=1)]\n",
    "\n",
    "# Select pandas rows based on list index\n",
    "index_list = [0,2]\n",
    "df2 = df.loc[df.index[index_list]]\n",
    "\n",
    "# Using DataFrame.iloc[] to select rows list index\n",
    "index_list = [0,2]\n",
    "df2 = df.iloc[index_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input list of lists (nested list)\n",
    "input_list = [[\"tutorialspoint\", \"python\"], [2, 6, 7], [9, 5, 12, 7]]\n",
    "print(input_list)\n",
    "\n",
    "# Getting each element from nested Lists and storing them in a new list using list comprehension\n",
    "resultList = [element for nestedlist in input_list for element in nestedlist]\n",
    "\n",
    "# printing the resultant list after joining the list of lists\n",
    "print(\"Resultant list after joining list of lists = \", resultList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "#Pandas provides a function called astype() to “categorize” string/text values. The function identifies each unique value that appears \n",
    "#in a column and stores it just once, and whenever the value reappears, only the reference is stored. For all of the highly repetitive \n",
    "#string/text data we have in our dataset, the RAM we’ll save will be quite substantial.\n",
    "df[col] = df[col].astype(CategoricalDtype(ordered=True))\n",
    "\n",
    "#Pandas provides a function called to_numeric() to “rightsize” integer and float values. Of course the numbers are already numeric, \n",
    "#so the key part of the function is the “downcast” parameter.\n",
    "\n",
    "df[open] = pd.to_numeric(df[open], downcast='float')\n",
    "df[fcols] = df[fcols].apply(pd.to_numeric, downcast='float')\n",
    "\n",
    "#downsize float64 to float32, or if it is numeric int64 to int32 to int16 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "%%time\n",
    "df = pd.read_csv(\"train.csv\", chunksize=1000)\n",
    "total_length = 0\n",
    "for chunk in df:\n",
    "    total_length += len(chunk)\n",
    "print(total_length)\n",
    "\n",
    "#######################################################################################################################################\n",
    "\n",
    "%%time\n",
    "LARGE_FILE = \"train.csv\"\n",
    "CHUNKSIZE = 1000 # processing 100,000 rows at a time\n",
    "\n",
    "def process_frame(df):\n",
    "        # process data frame\n",
    "        return len(df)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "        reader = pd.read_table(LARGE_FILE, chunksize=CHUNKSIZE)\n",
    "        pool = mp.Pool(4) # use 4 processes\n",
    "\n",
    "        funclist = []\n",
    "        for df in reader:\n",
    "                # process each data frame\n",
    "                f = pool.apply_async(process_frame,[df])\n",
    "                funclist.append(f)\n",
    "                \n",
    "        result = 0\n",
    "        for f in funclist:\n",
    "                result += f.get(timeout=10) # timeout in 10 seconds\n",
    "\n",
    "        print (f\"There are {result} rows of data\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import pandas as pd\n",
    "\n",
    "# Define the function to be applied to each group\n",
    "def process_group(group):\n",
    "    # Perform some processing on the group\n",
    "    # Replace this with your own group processing logic\n",
    "    processed_group = group.mean()  # Taking the mean as an example\n",
    "\n",
    "    return processed_group\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a sample DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'Group': ['A', 'A', 'B', 'B', 'C', 'C'],\n",
    "        'Value': [1, 2, 3, 4, 5, 6]\n",
    "    })\n",
    "\n",
    "    # Split the DataFrame into groups based on the 'Group' column\n",
    "    groups = df.groupby('Group')\n",
    "\n",
    "    # Define the number of processes to use (number of CPU cores)\n",
    "    num_processes = multiprocessing.cpu_count()\n",
    "\n",
    "    # Create a multiprocessing Pool with the specified number of processes\n",
    "    pool = multiprocessing.Pool(processes=num_processes)\n",
    "\n",
    "    # Apply the process_group function to each group in parallel using the multiprocessing Pool\n",
    "    results = pool.map(process_group, [group for _, group in groups])\n",
    "\n",
    "    # Close the pool to release resources\n",
    "    pool.close()\n",
    "\n",
    "    # Wait for all processes to complete\n",
    "    pool.join()\n",
    "\n",
    "    # Combine the processed results into a single DataFrame\n",
    "    processed_df = pd.concat(results)\n",
    "\n",
    "    # Print the processed DataFrame\n",
    "    print(processed_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import pandas as pd\n",
    "\n",
    "# Define the function to be applied to each row\n",
    "def process_row(row):\n",
    "    # Perform some processing on the row\n",
    "    # Replace this with your own row processing logic\n",
    "    processed_value = row['Value'] * 2  # Doubling the value as an example\n",
    "\n",
    "    return processed_value\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a sample DataFrame\n",
    "    df = pd.DataFrame({'Value': [1, 2, 3, 4, 5]})\n",
    "\n",
    "    # Define the number of processes to use (number of CPU cores)\n",
    "    num_processes = multiprocessing.cpu_count()\n",
    "\n",
    "    # Create a multiprocessing Pool with the specified number of processes\n",
    "    pool = multiprocessing.Pool(processes=num_processes)\n",
    "\n",
    "    # Parallelize the apply operation using the multiprocessing Pool\n",
    "    df['ProcessedValue'] = pool.map(process_row, df.itertuples(index=False))\n",
    "\n",
    "    # Close the pool to release resources\n",
    "    pool.close()\n",
    "\n",
    "    # Wait for all processes to complete\n",
    "    pool.join()\n",
    "\n",
    "    # Print the processed DataFrame\n",
    "    print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import pandas as pd\n",
    "\n",
    "# Define the function to be applied to each group\n",
    "def process_group(group):\n",
    "    # Perform some processing on the group\n",
    "    # Replace this with your own group processing logic\n",
    "    processed_group = group.mean()  # Taking the mean as an example\n",
    "\n",
    "    return processed_group\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a sample DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'Group': ['A', 'A', 'B', 'B', 'C', 'C'],\n",
    "        'Value': [1, 2, 3, 4, 5, 6]\n",
    "    })\n",
    "\n",
    "    # Split the DataFrame into groups based on the 'Group' column\n",
    "    groups = df.groupby('Group')\n",
    "\n",
    "    # Define the number of processes to use (number of CPU cores)\n",
    "    num_processes = multiprocessing.cpu_count()\n",
    "\n",
    "    # Create a multiprocessing Pool with the specified number of processes\n",
    "    pool = multiprocessing.Pool(processes=num_processes)\n",
    "\n",
    "    # Apply the process_group function to each group in parallel using the multiprocessing Pool\n",
    "    results = pool.map(process_group, [group for _, group in groups])\n",
    "\n",
    "    # Close the pool to release resources\n",
    "    pool.close()\n",
    "\n",
    "    # Wait for all processes to complete\n",
    "    pool.join()\n",
    "\n",
    "    # Combine the processed results into a single DataFrame\n",
    "    processed_df = pd.concat(results)\n",
    "\n",
    "    # Print the processed DataFrame\n",
    "    print(processed_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define a function to apply to each column of the DataFrame\n",
    "def process_column(column):\n",
    "    # Perform some computationally intensive operation on the column\n",
    "    # Replace this with your own column processing logic\n",
    "    result = np.mean(column)\n",
    "\n",
    "    return result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a sample DataFrame\n",
    "    df = pd.DataFrame({'Column1': [1, 2, 3], 'Column2': [4, 5, 6]})\n",
    "\n",
    "    # Define the number of processes to use (number of CPU cores)\n",
    "    num_processes = multiprocessing.cpu_count()\n",
    "\n",
    "    # Create a multiprocessing Pool with the specified number of processes\n",
    "    pool = multiprocessing.Pool(processes=num_processes)\n",
    "\n",
    "    # Apply the process_column function to each column in parallel using the multiprocessing Pool\n",
    "    results = pool.map(process_column, [df[column] for column in df.columns])\n",
    "\n",
    "    # Close the pool to release resources\n",
    "    pool.close()\n",
    "\n",
    "    # Wait for all processes to complete\n",
    "    pool.join()\n",
    "\n",
    "    # Create a new DataFrame with the processed results\n",
    "    processed_df = pd.DataFrame({'Column': df.columns, 'Result': results})\n",
    "\n",
    "    # Print the processed DataFrame\n",
    "    print(processed_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#And also columns such as Gender, etc.. can be stored as categorical values which reduces the memory from ~1000 KB to ~100 KB. (check the sats)\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "df['Thumb'].memory_usage(index=False, deep=True)\n",
    "#17014648\n",
    "df = pd.read_csv(\"train.csv\", dtype={\"Thumb\": \"category\"})\n",
    "df['Thumb'].memory_usage(index=False, deep=True)\n",
    "#401410"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#int8 can store integers from -128 to 127.\n",
    "#int16 can store integers from -32768 to 32767.\n",
    "#int64 can store integers from -9223372036854775808 to 9223372036854775807.\n",
    "#Pandas assign int64 to integer datatype by default, therefore by defining correct dtypes we can reduce memory usage significantly.\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "df['YearMade'].memory_usage(index=False, deep=True)\n",
    "#3209000\n",
    "df['YearMade'].min()\n",
    "#1000\n",
    "df['YearMade'].max()\n",
    "#2013\n",
    "df = pd.read_csv(\"train.csv\", dtype={\"YearMade\": \"int16\"})\n",
    "df['YearMade'].memory_usage(index=False, deep=True)\n",
    "#802250\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean = df.duplicated(subset=['Student']).any() # True\n",
    "# We were expecting True, as Joe can be seen twice.\n",
    "\n",
    "boolean = df.duplicated().any() # False\n",
    "boolean = df.duplicated(subset=['Student','Date']).any() # False\n",
    "\n",
    "#Approach 3: Use drop_duplicates method\n",
    "df.drop_duplicates(subset=['Student'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
